{"title":"Data Analytics Engineer - Google Cloud - REMOTE NEW!","location":"","salary":"","description":"","term":"$140,000 to $175,000 AnnuallyFull-Time","detail":"https://www.ziprecruiter.com/k/l/AAKjvzN1IpzKJpk0_xLBR3vGXFkWvD36f818727B6P_AlbN2sJ5pFe83lyqGANx0ubSC-6P6EtbjT0Qiv9RChhxBZHESI91QwWXTSnGYMzuKL1g9tJtPbugreJyeICz6vBLe-jG6AWB2x4ig9PDt1VR2zrpnQfleiUIKf9sGP6OGXxM-8eN0Bl-qM9TaoX0","source":"ziprecruiter","full_description":"Our TeamOur team is made up of experienced software and data engineers who solve the data needs of our organization. We work closely with both product engineers and data analysts, transforming broad goals into specific data requirements that we then deliver. We also work with engineers to develop solutions and answer questions regarding data. The expectations are high, but so is the support from management - promoting data integrity is a key mandate for the organization.Our WorkMuch of the Data Team’s work until now has been extraction of core data for internal consumption. With the launch of Analytics, we are now supplying data for customer consumption as well. In addition, our product organization’s gradual decomposition of our core application into microservices provides us with new challenges and opportunities in storing and migrating data.Our team has just lost its last remaining engineer from a previous era and we are determined to rewrite old code. Much of it is in Python, but core pieces are in Scala and Java and we would like to replace these with components we understand.We are heavily invested in Google’s cloud infrastructure but are in the process of shifting our data warehouse to Snowflake. Listed below is our stack. Experience in the following specific technologies is not required - we are looking for people who enjoy learning!SnowflakeGoogle Cloud (Bigquery, Dataflow, Dataproc, Composer, PubSub, Kubernetes)DBTPython (current)Java/Scala (deprecating)CircleCITerraform/SnowchangeYour WorkHelp integrate Snowflake into customer and internal analytic productsDevelop new product data ETLs to meet the needs of customer and internal analytic productsRewrite in Python core extractions currently written in Java and Scala, bringing our platform to one consistent languageWork with feature teams to establish data warehouse processes for streaming and reading data, especially as they decompose into event-based microservicesExplore new data technologies and advise teams on best practicesPreferred Education and Experience:Advanced programming abilityUnderstanding and application of modern data processing technology stacks, such as Spark, Hadoop ecosystem technologies, DataFlow, and othersUnderstanding of event-based architectures and streaming data technologiesExperience building ETL/ELT pipelines, familiar with AirflowUnderstanding of data warehousing in a cloud environmentIdeally, you also possess…Ability to program in Python and Java (with a bonus for PySpark experience)Experience with Google BigQuery or SnowflakeExperience with agile development methods"}